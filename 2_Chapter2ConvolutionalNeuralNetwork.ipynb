{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/farrelrassya/DeepLearning-A-Z-Python/blob/main/2_Chapter2ConvolutionalNeuralNetwork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DR-eO17geWu"
      },
      "source": [
        "# Convolutional Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMefrVPCg-60"
      },
      "source": [
        "### Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCV30xyVhFbE"
      },
      "source": [
        "import tensorflow as tf\n",
        "from keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIleuCAjoFD8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "03895422-b8dc-46d9-e7d1-3f818d27b463"
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.14.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ka45w0m_0TZ6",
        "outputId": "2987a681-ba97-405e-e42f-0ef95b7cdf0d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxQxCBWyoGPE"
      },
      "source": [
        "## Part 1 - Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvE-heJNo3GG"
      },
      "source": [
        "### Preprocessing the Training set"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Image data generator (train_datagen) is configured to preprocess and augment images from a specified directory. The rescaling factor of 1/255 normalizes pixel values, shear range, zoom range, and horizontal flip are applied as data augmentation techniques to diversify the training data. The flow_from_directory function is used to create a generator (training_set) that loads and augments images from the My Drive/Dataset/training_set directory, resizes them to 64x64 pixels, groups them in batches of 32, and labels them with a binary class mode, typically used for binary classification tasks."
      ],
      "metadata": {
        "id": "vwtixXLtRxYg"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0koUcJMJpEBD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c67616d-566f-4a26-ddb6-d1b5714c54e6"
      },
      "source": [
        "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   shear_range = 0.2,\n",
        "                                   zoom_range = 0.2,\n",
        "                                   horizontal_flip = True)\n",
        "training_set = train_datagen.flow_from_directory('/content/drive/My Drive/Dataset/training_set',\n",
        "                                                 target_size = (64, 64),\n",
        "                                                 batch_size = 32,\n",
        "                                                 class_mode = 'binary')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 8030 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrCMmGw9pHys"
      },
      "source": [
        "### Preprocessing the Test set"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Separate image data generator (test_datagen) is configured for processing test images. It applies pixel value rescaling with a factor of 1/255. The flow_from_directory function is then used to create a test data generator (test_set) that loads and resizes images from the My Drive/Dataset/test_set directory to 64x64 pixels, organizes them into batches of 32, and assigns binary class labels, typically used for binary classification testing or evaluation purposes."
      ],
      "metadata": {
        "id": "2FnwM1WKR-Am"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SH4WzfOhpKc3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15775c0f-e96a-4d81-87ad-e479fd122360"
      },
      "source": [
        "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "test_set = test_datagen.flow_from_directory('/content/drive/My Drive/Dataset/test_set',\n",
        "                                            target_size = (64, 64),\n",
        "                                            batch_size = 32,\n",
        "                                            class_mode = 'binary')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2000 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af8O4l90gk7B"
      },
      "source": [
        "## Part 2 - Building the CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ces1gXY2lmoX"
      },
      "source": [
        "### Initialising the CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initializes a Sequential model in TensorFlow, a popular deep learning framework. This model is used to create a neural network architecture by adding layers sequentially one after the other. Subsequent code should include layer definitions and configurations to build the specific architecture of the Convolutional Neural Network (CNN) for your machine learning or deep learning task."
      ],
      "metadata": {
        "id": "N-1CjW-ESPx7"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAUt4UMPlhLS"
      },
      "source": [
        "cnn = tf.keras.models.Sequential()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5YJj_XMl5LF"
      },
      "source": [
        "### Step 1 - Convolution"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\\text{Output}(x, y) = \\sum_{i=1}^{k} \\sum_{j=1}^{k} \\text{Input}(x + i, y + j) \\cdot \\text{Kernel}(i, j)$$\n",
        "\n",
        "represents the fundamental operation of feature extraction. It describes how a feature map element at position\n",
        "(x,y) is computed by summing the weighted values of neighboring pixels from the input image, where the weights are determined by the convolutional kernel. This operation is performed iteratively across the entire image, sliding the kernel to capture local patterns. In CNNs, these convolutions are crucial for learning and detecting features like edges, textures, and higher-level patterns, forming the basis for hierarchical feature representation and ultimately aiding in tasks such as image recognition and object detection."
      ],
      "metadata": {
        "id": "T-oAq56hZXml"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adds the first layer to a Convolutional Neural Network (CNN) model using TensorFlow and Keras. This layer is a convolutional layer with 32 filters of size 3x3, applying the Rectified Linear Unit (ReLU) activation function. It's designed to process input images of size 64x64 pixels with 3 color channels (RGB). This layer's purpose is to detect various features and patterns in the input images, serving as the initial building block of the CNN architecture, which can be further expanded by adding more layers for tasks such as image classification or object recognition."
      ],
      "metadata": {
        "id": "20H8eeWhSXAU"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPzPrMckl-hV"
      },
      "source": [
        "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[64, 64, 3]))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tf87FpvxmNOJ"
      },
      "source": [
        "### Step 2 - Pooling"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adds a MaxPooling layer to a Convolutional Neural Network (CNN) model using TensorFlow and Keras. This MaxPooling layer is configured to reduce the spatial dimensions of the feature maps produced by previous convolutional layers. It employs a 2x2 pooling window to capture the maximum value from each 2x2 block of the input feature map and shifts this window by 2 pixels at a time. MaxPooling helps in retaining important features while decreasing the computational burden of the model. In a typical CNN architecture, this layer is often followed by additional convolutional layers to create a hierarchical representation of features, which is commonly used for image recognition and classification tasks."
      ],
      "metadata": {
        "id": "fpsP8U16Sg_D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$${Output}(x, y) = \\max \\left(\\text{Input}(x', y') \\text{ for all } x', y' \\text{ in the pooling region} \\right) $$\n"
      ],
      "metadata": {
        "id": "-W8R8qOpaaJl"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncpqPl69mOac"
      },
      "source": [
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaTOgD8rm4mU"
      },
      "source": [
        "### Adding a second convolutional layer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adds two layers to a Convolutional Neural Network (CNN) using TensorFlow and Keras. The first line adds a convolutional layer with 32 filters of size 3x3 and a Rectified Linear Unit (ReLU) activation function, extracting features from the input data. The second line adds a MaxPooling layer with a 2x2 pooling window and a 2-pixel stride, reducing the spatial dimensions of the feature maps. These layers are common in CNN architectures for tasks like image recognition and classification, typically followed by more layers to complete the network."
      ],
      "metadata": {
        "id": "IJnqSR67St1U"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_-FZjn_m8gk"
      },
      "source": [
        "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmiEuvTunKfk"
      },
      "source": [
        "### Step 3 - Flattening"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adds a Flatten layer to the Convolutional Neural Network (CNN) model. This layer is essential for transitioning from the convolutional and pooling layers to the fully connected layers in the network. It reshapes the 2D feature maps from the previous layers into a 1D vector, enabling them to be connected to a traditional feedforward neural network structure. This is often a crucial step in CNN architectures when preparing the data for classification or regression tasks that require fully connected layers."
      ],
      "metadata": {
        "id": "AIKy2wk4SzkC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$ {Flatten}(\\text{Input}) = \\text{Input}(x, y) \\text{ for all } x, y $$\n"
      ],
      "metadata": {
        "id": "vmmsjLVHavjj"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AZeOGCvnNZn"
      },
      "source": [
        "cnn.add(tf.keras.layers.Flatten())"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAoSECOm203v"
      },
      "source": [
        "### Step 4 - Full Connection"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adds a Dense layer to the Convolutional Neural Network (CNN) model. This Dense layer has 128 units and uses the Rectified Linear Unit (ReLU) activation function. In a CNN, Dense layers are typically used for making final predictions or decisions based on the features extracted by the earlier convolutional and pooling layers. The 128 units represent neurons in the layer, and the ReLU activation introduces non-linearity into the network, helping it capture complex patterns in the data. This layer is often followed by another Dense layer and an output layer with appropriate activation functions to suit the specific machine learning or deep learning task, such as image classification."
      ],
      "metadata": {
        "id": "w5uGnUuqS63I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$ {ReLU}(x) = \\max(0, x) $$"
      ],
      "metadata": {
        "id": "d69Xcnbna_gm"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GtmUlLd26Nq"
      },
      "source": [
        "cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTldFvbX28Na"
      },
      "source": [
        "### Step 5 - Output Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code adds a Dense layer to the Convolutional Neural Network (CNN) model. This Dense layer has a single unit and uses the sigmoid activation function. In the context of a binary classification task, this layer is often the output layer, where the sigmoid activation function is used to produce a probability score between 0 and 1, indicating the likelihood of the input belonging to one of the two classes (e.g., 0 for class A and 1 for class B). This layer is responsible for making the final prediction based on the features extracted by the earlier layers in the CNN."
      ],
      "metadata": {
        "id": "XvKSdQvJS_kN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$ {Sigmoid}(x) = \\frac{1}{1 + e^{-x}} $$\n"
      ],
      "metadata": {
        "id": "Y-fI-XqPa7vp"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1p_Zj1Mc3Ko_"
      },
      "source": [
        "cnn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6XkI90snSDl"
      },
      "source": [
        "## Part 3 - Training the CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfrFQACEnc6i"
      },
      "source": [
        "### Compiling the CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adam Equation:\n",
        "$$\n",
        "\\theta_{t+1} = \\theta_{t} - \\frac{\\alpha}{\\sqrt{v_t} + \\epsilon} \\cdot m_t\n",
        "$$\n",
        "\n",
        "\n",
        "The components in the Adam optimization equation can be described in mathematical terms as follows: θ_(t+1) represents the updated model parameters at the time step (t+1), while θ_t signifies the model parameters at the current time step, t. The learning rate, denoted as \"alpha,\" is a critical factor that regulates the size of parameter updates during optimization. The variables m_t and v_t act as estimates of the first and second moments, respectively. These moments are computed using exponential moving averages of gradients and squared gradients, providing valuable insights into the trends and variations within the data. Lastly, \"epsilon\" is a small constant systematically introduced into the equations' denominators to prevent division by zero, enhancing numerical stability in the optimization process. Collectively, these elements constitute the foundation of the Adam optimizer, enabling adaptive and efficient adjustments of learning rates and making it highly effective for training complex neural networks.\n"
      ],
      "metadata": {
        "id": "15qfGVDLTeT7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configuration for training Convolutional Neural Networks (CNNs) in binary classification tasks. The 'adam' optimizer offers efficient weight updates, 'binary_crossentropy' is a well-suited loss function for binary classification, and 'accuracy' provides a straightforward metric for evaluating classification performance. While these choices are widely adopted as strong starting points, it's important to consider that the most suitable configurations can vary depending on the specific dataset and problem, often requiring further fine-tuning and experimentation for optimal results."
      ],
      "metadata": {
        "id": "GLBk1W9wTLOJ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NALksrNQpUlJ"
      },
      "source": [
        "cnn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehS-v3MIpX2h"
      },
      "source": [
        "### Training the CNN on the Training set and evaluating it on the Test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUj1W4PJptta",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3c75184-0b6f-472c-a790-68d6c7e24dbf"
      },
      "source": [
        "cnn.fit(x = training_set, validation_data = test_set, epochs = 25)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "251/251 [==============================] - 1726s 7s/step - loss: 0.6716 - accuracy: 0.5861 - val_loss: 0.6308 - val_accuracy: 0.6715\n",
            "Epoch 2/25\n",
            "251/251 [==============================] - 51s 203ms/step - loss: 0.6038 - accuracy: 0.6654 - val_loss: 0.5889 - val_accuracy: 0.6945\n",
            "Epoch 3/25\n",
            "251/251 [==============================] - 51s 203ms/step - loss: 0.5725 - accuracy: 0.6985 - val_loss: 0.5533 - val_accuracy: 0.7230\n",
            "Epoch 4/25\n",
            "251/251 [==============================] - 48s 192ms/step - loss: 0.5326 - accuracy: 0.7310 - val_loss: 0.5148 - val_accuracy: 0.7585\n",
            "Epoch 5/25\n",
            "251/251 [==============================] - 54s 214ms/step - loss: 0.5133 - accuracy: 0.7493 - val_loss: 0.5115 - val_accuracy: 0.7485\n",
            "Epoch 6/25\n",
            "251/251 [==============================] - 52s 208ms/step - loss: 0.4981 - accuracy: 0.7563 - val_loss: 0.4875 - val_accuracy: 0.7640\n",
            "Epoch 7/25\n",
            "251/251 [==============================] - 49s 197ms/step - loss: 0.4654 - accuracy: 0.7780 - val_loss: 0.5062 - val_accuracy: 0.7575\n",
            "Epoch 8/25\n",
            "251/251 [==============================] - 53s 212ms/step - loss: 0.4557 - accuracy: 0.7837 - val_loss: 0.4954 - val_accuracy: 0.7675\n",
            "Epoch 9/25\n",
            "251/251 [==============================] - 52s 207ms/step - loss: 0.4441 - accuracy: 0.7917 - val_loss: 0.4642 - val_accuracy: 0.7815\n",
            "Epoch 10/25\n",
            "251/251 [==============================] - 50s 201ms/step - loss: 0.4216 - accuracy: 0.8047 - val_loss: 0.4570 - val_accuracy: 0.7980\n",
            "Epoch 11/25\n",
            "251/251 [==============================] - 53s 211ms/step - loss: 0.3992 - accuracy: 0.8125 - val_loss: 0.4780 - val_accuracy: 0.7955\n",
            "Epoch 12/25\n",
            "251/251 [==============================] - 52s 205ms/step - loss: 0.4041 - accuracy: 0.8152 - val_loss: 0.4527 - val_accuracy: 0.7940\n",
            "Epoch 13/25\n",
            "251/251 [==============================] - 48s 190ms/step - loss: 0.3754 - accuracy: 0.8284 - val_loss: 0.4776 - val_accuracy: 0.7930\n",
            "Epoch 14/25\n",
            "251/251 [==============================] - 48s 191ms/step - loss: 0.3606 - accuracy: 0.8401 - val_loss: 0.5165 - val_accuracy: 0.7595\n",
            "Epoch 15/25\n",
            "251/251 [==============================] - 48s 190ms/step - loss: 0.3519 - accuracy: 0.8427 - val_loss: 0.4591 - val_accuracy: 0.8005\n",
            "Epoch 16/25\n",
            "251/251 [==============================] - 48s 191ms/step - loss: 0.3420 - accuracy: 0.8488 - val_loss: 0.4525 - val_accuracy: 0.7940\n",
            "Epoch 17/25\n",
            "251/251 [==============================] - 48s 193ms/step - loss: 0.3263 - accuracy: 0.8585 - val_loss: 0.5582 - val_accuracy: 0.7655\n",
            "Epoch 18/25\n",
            "251/251 [==============================] - 48s 192ms/step - loss: 0.2991 - accuracy: 0.8745 - val_loss: 0.4616 - val_accuracy: 0.8040\n",
            "Epoch 19/25\n",
            "251/251 [==============================] - 47s 189ms/step - loss: 0.2926 - accuracy: 0.8719 - val_loss: 0.4823 - val_accuracy: 0.8085\n",
            "Epoch 20/25\n",
            "251/251 [==============================] - 47s 187ms/step - loss: 0.2833 - accuracy: 0.8767 - val_loss: 0.4950 - val_accuracy: 0.7985\n",
            "Epoch 21/25\n",
            "251/251 [==============================] - 53s 211ms/step - loss: 0.2611 - accuracy: 0.8893 - val_loss: 0.4951 - val_accuracy: 0.8135\n",
            "Epoch 22/25\n",
            "251/251 [==============================] - 53s 212ms/step - loss: 0.2597 - accuracy: 0.8929 - val_loss: 0.5638 - val_accuracy: 0.7775\n",
            "Epoch 23/25\n",
            "251/251 [==============================] - 49s 197ms/step - loss: 0.2427 - accuracy: 0.9027 - val_loss: 0.5082 - val_accuracy: 0.8155\n",
            "Epoch 24/25\n",
            "251/251 [==============================] - 48s 191ms/step - loss: 0.2310 - accuracy: 0.9027 - val_loss: 0.5006 - val_accuracy: 0.8110\n",
            "Epoch 25/25\n",
            "251/251 [==============================] - 52s 208ms/step - loss: 0.2183 - accuracy: 0.9087 - val_loss: 0.5228 - val_accuracy: 0.8125\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7b507a0f2020>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3PZasO0006Z"
      },
      "source": [
        "## Part 4 - Making a single prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsSiWEJY1BPB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93d0ebf5-371f-4445-cb08-c970854e07d1"
      },
      "source": [
        "import numpy as np\n",
        "from keras.preprocessing import image\n",
        "test_image = image.load_img('/content/drive/My Drive/Dataset/single_prediction/cat_or_dog_1.jpg', target_size = (64, 64))\n",
        "test_image = image.img_to_array(test_image)\n",
        "test_image = np.expand_dims(test_image, axis = 0)\n",
        "result = cnn.predict(test_image)\n",
        "training_set.class_indices\n",
        "if result[0][0] == 1:\n",
        "  prediction = 'dog'\n",
        "else:\n",
        "  prediction = 'cat'"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 23ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ED9KB3I54c1i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de90a544-11e9-4dd3-ec91-5d844c150e96"
      },
      "source": [
        "print(prediction)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dog\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QXUjQkSpRiFc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}